{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "zCzG49Hhcill"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"Anthracnose\",\n",
        "    1: \"Banana Fruit-Scarring Beetle\",\n",
        "    2: \"Banana Skipper Damage\",\n",
        "    3: 'Banana Split Peel',\n",
        "    4: \"Black and Yellow Sigatoka\",\n",
        "    5: \"Chewing insect damage on banana leaf\",\n",
        "    6: \"Healthy Banana\",\n",
        "    7: \"Healthy Banana leaf\",\n",
        "    8: \"Panama Wilt Disease\",\n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "x2y6-s3gvCJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_path = '/content/drive/MyDrive/bananadata/AUGMENTED/data'\n",
        "build_path = '/content/drive/MyDrive/bananadata/build/augs'"
      ],
      "metadata": {
        "id": "hLRBVwxXcllF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10 # Try increasing epochs to 30\n",
        "LEARNING_RATE = 0.01\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 9\n",
        "IMAGE_SIZE = 224 # Transform the image and make the size go to 224\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEADS = 8 # INcrease the number heads\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "L-QHBk1Lt-5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.5, 0.5, 0.5],\n",
        "    )\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "    ])"
      ],
      "metadata": {
        "id": "GMVpqrNwtrfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = datasets.ImageFolder(root=dataset_path+'/train', transform=transform)\n",
        "dataset_val = datasets.ImageFolder(root=dataset_path+'/validation', transform=transform)\n",
        "dataset_test = datasets.ImageFolder(root=dataset_path+'/test', transform=transform)\n",
        "build_train = datasets.ImageFolder(root=build_path, transform=transform)"
      ],
      "metadata": {
        "id": "a3gOWtFSt0de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "load_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "load_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
        "load_build = DataLoader(build_train, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "wXpntEAAt33o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISION TRANSFORMER"
      ],
      "metadata": {
        "id": "smo2hCl3ugWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 patch_size,\n",
        "                 in_channels,\n",
        "                 embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=embed_dim,\n",
        "                              kernel_size=patch_size,\n",
        "                              stride=patch_size)\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        B = x.size(0)\n",
        "        x = self.proj(x) # (B, E, H/P, W/P)\n",
        "        x = x.flatten(2).transpose(1, 2) # (B, N, E)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        return x"
      ],
      "metadata": {
        "id": "GCqqKQBpuVSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features,\n",
        "                 drop_rate):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=in_features,\n",
        "                             out_features=hidden_features)\n",
        "        self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                             out_features=in_features)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.gelu(self.fc1(x)))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XatMHInEumpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "s_EKGrSeusrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.encoder = nn.Sequential(*[\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        return self.head(cls_token)\n",
        ""
      ],
      "metadata": {
        "id": "tCUtFdxSu1RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
        ").to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Measure how wrong our model is\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), # update our model's parameters to try and reduce the loss\n",
        "                             lr=LEARNING_RATE)\n",
        "total_step = len(load_train)"
      ],
      "metadata": {
        "id": "GF8B-dOku9wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(load_train):\n",
        "        # Move tensors to the configured device\n",
        "        images = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "    train_loss = running_loss / len(load_train.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in load_val:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "        val_loss = running_loss / len(load_val.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_accuracies.append(val_acc)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(945, 100 * correct / total)"
      ],
      "metadata": {
        "id": "QCvt_rDAvHJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZsvZv_CQ5Jg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "tDpZf1hI5N7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_y_true = []\n",
        "test_y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in load_test:\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        del images, labels, outputs"
      ],
      "metadata": {
        "id": "8sOzqEhy5R3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the network on the {} test images: {} %'.format(len(test_ds), 100 * correct / total))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_map.values(), yticklabels=labels_map.values())\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UlG_qCme5dwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "build_y_true = []\n",
        "build_y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # change to buuild dataset\n",
        "    for images, labels in load_build:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "id": "goCKupyP5jXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define the directory to save the results\n",
        "results_dir = \"/content/drive/MyDrive/bananadata/results/vit\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Define hyperparameters and results\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"epoch\": num_epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"SGD\",\n",
        "    \"loss_function\": \"CrossEntropyLoss\",\n",
        "    \"model_architecture\": \"VGG16\"\n",
        "}\n",
        "\n",
        "results = {\n",
        "    \"train_losses\": train_losses,\n",
        "    \"val_losses\": val_losses,\n",
        "    # Assuming you calculated training and validation accuracies and stored them in train_accuracies and val_accuracies lists\n",
        "    \"val_accuracies\": val_accuracies,\n",
        "    \"test_y_true\": test_y_true,\n",
        "    \"test_y_pred\": test_y_pred,\n",
        "    \"build_y_true\": build_y_true,\n",
        "    \"build_y_pred\": build_y_pred,\n",
        "}\n",
        "\n",
        "test_index = 1\n",
        "# Save hyperparameters\n",
        "hyperparameters_path = os.path.join(results_dir, f\"hyperparameters{test_index}.json\")\n",
        "with open(hyperparameters_path, \"w\") as f:\n",
        "    json.dump(hyperparameters, f, indent=4)\n",
        "print(f\"Hyperparameters saved to {hyperparameters_path}\")\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(results_dir, f\"results{test_index}.json\")\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "print(f\"Results saved to {results_path}\")"
      ],
      "metadata": {
        "id": "CIWoIAwL5qR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}