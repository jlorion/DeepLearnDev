{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCzG49Hhcill"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2y6-s3gvCJL"
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"Anthracnose\",\n",
    "    1: \"Banana Fruit-Scarring Beetle\",\n",
    "    2: \"Banana Skipper Damage\",\n",
    "    3: 'Banana Split Peel',\n",
    "    4: \"Black and Yellow Sigatoka\",\n",
    "    5: \"Chewing insect damage on banana leaf\",\n",
    "    6: \"Healthy Banana\",\n",
    "    7: \"Healthy Banana leaf\",\n",
    "    8: \"Panama Wilt Disease\",\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLRBVwxXcllF"
   },
   "outputs": [],
   "source": [
    "# change \n",
    "dataset_path = '/content/drive/MyDrive/bananadata/AUGMENTED/data'\n",
    "build_path = '/content/drive/MyDrive/bananadata/build/augs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-QHBk1Lt-5g"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10 # Try increasing epochs to 30\n",
    "LEARNING_RATE = 0.01\n",
    "PATCH_SIZE = 16\n",
    "NUM_CLASSES = 9\n",
    "IMAGE_SIZE = 224 # Transform the image and make the size go to 224\n",
    "CHANNELS = 3\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8 # INcrease the number heads\n",
    "DEPTH = 6\n",
    "MLP_DIM = 512\n",
    "DROP_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMVpqrNwtrfu"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5],\n",
    "    )\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3gOWtFSt0de"
   },
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(root=dataset_path+'/train', transform=transform)\n",
    "dataset_val = datasets.ImageFolder(root=dataset_path+'/validation', transform=transform)\n",
    "dataset_test = datasets.ImageFolder(root=dataset_path+'/test', transform=transform)\n",
    "build_train = datasets.ImageFolder(root=build_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXpntEAAt33o"
   },
   "outputs": [],
   "source": [
    "load_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "load_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "load_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "load_build = DataLoader(build_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smo2hCl3ugWs"
   },
   "source": [
    "# VISION TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCqqKQBpuVSE"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size,\n",
    "                 patch_size,\n",
    "                 in_channels,\n",
    "                 embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.size(0)\n",
    "        x = self.proj(x) # (B, E, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1, 2) # (B, N, E)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XatMHInEumpH"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features,\n",
    "                             out_features=hidden_features)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features,\n",
    "                             out_features=in_features)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.gelu(self.fc1(x)))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_EKGrSeusrw"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCUtFdxSu1RY"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token = x[:, 0]\n",
    "        return self.head(cls_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF8B-dOku9wL"
   },
   "outputs": [],
   "source": [
    "model = VisionTransformer(\n",
    "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
    "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
    ").to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Measure how wrong our model is\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), # update our model's parameters to try and reduce the loss\n",
    "                             lr=LEARNING_RATE)\n",
    "total_step = len(load_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCvt_rDAvHJ6"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(load_train):\n",
    "        # Move tensors to the configured device\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(load_train.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Validation\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in load_val:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss / len(load_val.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = 100 * correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(945, 100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsvZv_CQ5Jg9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDpZf1hI5N7O"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sOzqEhy5R3_"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_y_true = []\n",
    "test_y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in load_test:\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        del images, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlG_qCme5dwg"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the {} test images: {} %'.format(len(test_ds), 100 * correct / total))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_map.values(), yticklabels=labels_map.values())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goCKupyP5jXB"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "build_y_true = []\n",
    "build_y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # change to buuild dataset\n",
    "    for images, labels in load_build:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIWoIAwL5qR5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the directory to save the results\n",
    "results_dir = \"/content/drive/MyDrive/bananadata/results/vit\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define hyperparameters and results\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epoch\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"model_architecture\": \"VGG16\"\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    # Assuming you calculated training and validation accuracies and stored them in train_accuracies and val_accuracies lists\n",
    "    \"val_accuracies\": val_accuracies,\n",
    "    \"test_y_true\": test_y_true,\n",
    "    \"test_y_pred\": test_y_pred,\n",
    "    \"build_y_true\": build_y_true,\n",
    "    \"build_y_pred\": build_y_pred,\n",
    "}\n",
    "\n",
    "test_index = 1\n",
    "# Save hyperparameters\n",
    "hyperparameters_path = os.path.join(results_dir, f\"hyperparameters{test_index}.json\")\n",
    "with open(hyperparameters_path, \"w\") as f:\n",
    "    json.dump(hyperparameters, f, indent=4)\n",
    "print(f\"Hyperparameters saved to {hyperparameters_path}\")\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(results_dir, f\"results{test_index}.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
